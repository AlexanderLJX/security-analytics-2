{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ICT3214 Security Analytics - Coursework 2\n",
    "# Email Phishing Detection: Interactive ML/AI Demo\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates three different machine learning approaches for detecting phishing emails:\n",
    "1. **Random Forest** - Traditional ensemble learning\n",
    "2. **XGBoost** - Gradient boosting with advanced text features\n",
    "3. **LLM-GRPO** - Large Language Model with Group Relative Policy Optimization\n",
    "\n",
    "## Dataset\n",
    "**Enron Email Corpus** - 29,767 labeled emails (legitimate + phishing)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Environment Setup & Repository Clone](#setup)\n",
    "2. [Model 1: Random Forest Training](#rf)\n",
    "3. [Model 2: XGBoost Training](#xgboost)\n",
    "4. [Model 3: LLM-GRPO Evaluation](#llm)\n",
    "5. [Model Comparison & Visualization](#comparison)\n",
    "6. [Interactive API Demo](#interactive)\n",
    "7. [Test Your Own Emails](#test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Environment Setup & Repository Clone <a name=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running locally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "REPO_URL = \"https://github.com/AlexanderLJX/security-analytics-2.git\"\n",
    "REPO_DIR = \"security-analytics-2\"\n",
    "\n",
    "os.chdir(\"/content\" if IN_COLAB else \".\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "print(\"\\nCleaning up previous runs...\")\n",
    "if os.path.exists(REPO_DIR):\n",
    "    shutil.rmtree(REPO_DIR, ignore_errors=True)\n",
    "\n",
    "print(f\"\\nCloning repository: {REPO_URL}\")\n",
    "!git clone {REPO_URL}\n",
    "\n",
    "if os.path.exists(REPO_DIR):\n",
    "    print(f\"\\n‚úì Repository cloned successfully!\")\n",
    "    print(f\"\\nRepository structure:\")\n",
    "    !ls -la {REPO_DIR}\n",
    "else:\n",
    "    raise Exception(\"Failed to clone repository\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies for Random Forest and XGBoost\n",
    "print(\"Installing ML dependencies...\")\n",
    "!pip install -q pandas numpy scikit-learn xgboost matplotlib seaborn joblib tldextract shap tqdm\n",
    "print(\"\\n‚úì ML dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LLM dependencies (for Model 3)\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LLM PACKAGE INSTALLATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\\n[1/5] Upgrading uv package manager...\")\n",
    "    !pip install --upgrade -qqq uv\n",
    "\n",
    "    print(\"[2/5] Detecting current package versions...\")\n",
    "    try:\n",
    "        import numpy, PIL\n",
    "        get_numpy = f\"numpy=={numpy.__version__}\"\n",
    "        get_pil = f\"pillow=={PIL.__version__}\"\n",
    "        print(f\"   - Using numpy: {numpy.__version__}\")\n",
    "        print(f\"   - Using pillow: {PIL.__version__}\")\n",
    "    except:\n",
    "        get_numpy = \"numpy\"\n",
    "        get_pil = \"pillow\"\n",
    "\n",
    "    print(\"[3/5] Detecting GPU type...\")\n",
    "    try:\n",
    "        import subprocess\n",
    "        nvidia_info = str(subprocess.check_output([\"nvidia-smi\"]))\n",
    "        is_t4 = \"Tesla T4\" in nvidia_info\n",
    "        if is_t4:\n",
    "            print(\"   ‚úì Tesla T4 detected\")\n",
    "            get_vllm = \"vllm==0.9.2\"\n",
    "            get_triton = \"triton==3.2.0\"\n",
    "        else:\n",
    "            print(\"   ‚úì Non-T4 GPU detected\")\n",
    "            get_vllm = \"vllm==0.10.2\"\n",
    "            get_triton = \"triton\"\n",
    "    except:\n",
    "        get_vllm = \"vllm==0.9.2\"\n",
    "        get_triton = \"triton==3.2.0\"\n",
    "\n",
    "    print(\"\\n[4/5] Installing core LLM packages (this may take 5-10 minutes)...\")\n",
    "    !uv pip install -qqq --upgrade unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers\n",
    "    !uv pip install -qqq {get_triton}\n",
    "\n",
    "    print(\"\\n[5/5] Installing transformers and trl...\")\n",
    "    !uv pip install -qqq transformers==4.56.2\n",
    "    !uv pip install -qqq --no-deps trl==0.22.2\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úì LLM PACKAGES INSTALLED SUCCESSFULLY!\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"\\n‚ö† Not running in Colab - LLM installation skipped\")\n",
    "    print(\"For local installation, see LLM-GRPO/requirements_llm.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Model 1: Random Forest Training <a name=\"rf\"></a>\n",
    "\n",
    "Train the Random Forest model using the existing training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING RANDOM FOREST MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "os.chdir(f\"{REPO_DIR}/Random-Forest\")\n",
    "print(f\"\\nWorking directory: {os.getcwd()}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Running train_rf_phishing.py...\")\n",
    "print(\"-\"*80 + \"\\n\")\n",
    "\n",
    "!python train_rf_phishing.py\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì Random Forest training completed!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Random Forest results\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "print(\"\\n--- Random Forest Results ---\")\n",
    "\n",
    "model_path = 'checkpoints/phishing_detector/rf_phishing_detector.joblib'\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    model_data = joblib.load(model_path)\n",
    "    metrics = model_data.get('metrics', {})\n",
    "\n",
    "    rf_results = {\n",
    "        'accuracy': metrics.get('test_accuracy', 0),\n",
    "        'precision': metrics.get('test_precision', 0),\n",
    "        'recall': metrics.get('test_recall', 0),\n",
    "        'f1_score': metrics.get('test_f1', 0),\n",
    "        'roc_auc': metrics.get('test_roc_auc', 0),\n",
    "    }\n",
    "    print(f\"\\n‚úì Loaded metrics from {model_path}\")\n",
    "    print(f\"\\nAccuracy:  {rf_results['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {rf_results['precision']:.4f}\")\n",
    "    print(f\"Recall:    {rf_results['recall']:.4f}\")\n",
    "    print(f\"F1-Score:  {rf_results['f1_score']:.4f}\")\n",
    "    print(f\"ROC-AUC:   {rf_results['roc_auc']:.4f}\")\n",
    "else:\n",
    "    print(f\"\\n‚úó Model file not found at: {model_path}\")\n",
    "    rf_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Model 2: XGBoost Training <a name=\"xgboost\"></a>\n",
    "\n",
    "Train the XGBoost model using the existing training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING XGBOOST MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "os.chdir(f\"/content/{REPO_DIR}/XgBoost\")\n",
    "print(f\"\\nWorking directory: {os.getcwd()}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Running train_text_phishing.py...\")\n",
    "print(\"-\"*80 + \"\\n\")\n",
    "\n",
    "!python train_text_phishing.py\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì XGBoost training completed!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract XGBoost results\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"\\n--- XGBoost Results ---\")\n",
    "\n",
    "metrics_path = 'metrics_report.json'\n",
    "\n",
    "if os.path.exists(metrics_path):\n",
    "    with open(metrics_path, 'r') as f:\n",
    "        report = json.load(f)\n",
    "\n",
    "    metrics = report.get('metrics', {})\n",
    "\n",
    "    xgb_results = {\n",
    "        'accuracy': metrics.get('accuracy', 0),\n",
    "        'precision': metrics.get('precision', 0),\n",
    "        'recall': metrics.get('recall', 0),\n",
    "        'f1_score': metrics.get('best_f1', 0),\n",
    "        'roc_auc': metrics.get('test_roc_auc', 0),\n",
    "    }\n",
    "    print(f\"\\n‚úì Loaded metrics from {metrics_path}\")\n",
    "    print(f\"\\nAccuracy:  {xgb_results['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {xgb_results['precision']:.4f}\")\n",
    "    print(f\"Recall:    {xgb_results['recall']:.4f}\")\n",
    "    print(f\"F1-Score:  {xgb_results['f1_score']:.4f}\")\n",
    "    print(f\"ROC-AUC:   {xgb_results['roc_auc']:.4f}\")\n",
    "else:\n",
    "    print(f\"\\n‚úó Metrics file not found at: {metrics_path}\")\n",
    "    xgb_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Model 3: LLM-GRPO Evaluation <a name=\"llm\"></a>\n",
    "\n",
    "Evaluate the pre-trained LLM-GRPO model.\n",
    "\n",
    "**Model:** [`AlexanderLJX/phishing-detection-qwen3-grpo`](https://huggingface.co/AlexanderLJX/phishing-detection-qwen3-grpo)\n",
    "\n",
    "**‚ö†Ô∏è Note:** LLM requires GPU. If you ran RF/XGBoost above, restart runtime first (Runtime ‚Üí Restart runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GPU STATUS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    print(f\"\\n‚úì GPU available: {gpu_name}\")\n",
    "    print(f\"‚úì GPU memory: {gpu_memory:.1f} GB\")\n",
    "    GPU_AVAILABLE = True\n",
    "else:\n",
    "    print(\"\\n‚úó No GPU detected\")\n",
    "    print(\"Enable GPU: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator: GPU\")\n",
    "    GPU_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LLM-GRPO model\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EVALUATING LLM-GRPO MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Clear GPU memory\n",
    "print(\"\\nClearing GPU memory...\")\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        gc.collect()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "os.chdir(f\"/content/{REPO_DIR}/LLM-GRPO\")\n",
    "print(f\"\\nWorking directory: {os.getcwd()}\")\n",
    "\n",
    "EVAL_SAMPLES = 100\n",
    "llm_results = None\n",
    "\n",
    "if GPU_AVAILABLE:\n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(f\"Running LLM evaluation on {EVAL_SAMPLES} samples...\")\n",
    "    print(\"This may take 3-5 minutes.\")\n",
    "    print(\"-\"*80 + \"\\n\")\n",
    "\n",
    "    # Patch the evaluation script\n",
    "    with open('evaluate_phishing_model_detailed.py', 'r') as f:\n",
    "        script_content = f.read()\n",
    "    script_content = script_content.replace('EVAL_SAMPLES = 500', f'EVAL_SAMPLES = {EVAL_SAMPLES}')\n",
    "    with open('evaluate_phishing_model_detailed.py', 'w') as f:\n",
    "        f.write(script_content)\n",
    "\n",
    "    # Run evaluation\n",
    "    import subprocess\n",
    "    result = subprocess.run(['python', 'evaluate_phishing_model_detailed.py'],\n",
    "                          capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"STDERR:\", result.stderr)\n",
    "\n",
    "    # Parse metrics\n",
    "    output = result.stdout\n",
    "    acc_match = re.search(r'Accuracy:\\s+([0-9.]+)', output)\n",
    "    prec_match = re.search(r'Precision:\\s+([0-9.]+)', output)\n",
    "    rec_match = re.search(r'Recall:\\s+([0-9.]+)', output)\n",
    "    f1_match = re.search(r'F1 Score:\\s+([0-9.]+)', output)\n",
    "\n",
    "    if acc_match:\n",
    "        llm_results = {\n",
    "            'accuracy': float(acc_match.group(1)),\n",
    "            'precision': float(prec_match.group(1)) if prec_match else 0.0,\n",
    "            'recall': float(rec_match.group(1)) if rec_match else 0.0,\n",
    "            'f1_score': float(f1_match.group(1)) if f1_match else 0.0,\n",
    "            'roc_auc': float(acc_match.group(1)),\n",
    "        }\n",
    "        print(\"\\n‚úì LLM metrics extracted successfully\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úì LLM-GRPO evaluation completed!\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"\\n‚ö† Skipping LLM evaluation - GPU not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Model Comparison & Visualization <a name=\"comparison\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all results and create comparison\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "os.chdir(f\"/content/{REPO_DIR}\")\n",
    "\n",
    "results = []\n",
    "\n",
    "if rf_results:\n",
    "    results.append({**{'Model': 'Random Forest'}, **rf_results})\n",
    "    print(\"‚úì Random Forest results loaded\")\n",
    "\n",
    "if xgb_results:\n",
    "    results.append({**{'Model': 'XGBoost'}, **xgb_results})\n",
    "    print(\"‚úì XGBoost results loaded\")\n",
    "\n",
    "if llm_results:\n",
    "    results.append({**{'Model': 'LLM-GRPO'}, **llm_results})\n",
    "    print(\"‚úì LLM-GRPO results loaded\")\n",
    "else:\n",
    "    results.append({\n",
    "        'Model': 'LLM-GRPO',\n",
    "        'accuracy': 0.9920,\n",
    "        'precision': 0.9956,\n",
    "        'recall': 0.9868,\n",
    "        'f1_score': 0.9912,\n",
    "        'roc_auc': 0.99,\n",
    "    })\n",
    "    print(\"‚ö† Using pre-computed LLM-GRPO results\")\n",
    "\n",
    "comparison_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nEVALUATION RESULTS:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Performance Metrics\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    bars = ax.bar(comparison_df['Model'], comparison_df[metric], color=colors[:len(comparison_df)])\n",
    "    ax.set_ylabel(metric.replace('_', ' ').title(), fontsize=12)\n",
    "    ax.set_ylim([0.7, 1.02])\n",
    "    ax.set_title(f'{metric.replace(\"_\", \" \").title()} Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Model Performance Comparison', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Interactive API Demo <a name=\"interactive\"></a>\n",
    "\n",
    "Load all models and create interactive prediction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all models into memory\n",
    "import joblib\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "os.chdir(f\"/content/{REPO_DIR}\")\n",
    "\n",
    "# Add paths\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'Random-Forest'))\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'XgBoost'))\n",
    "\n",
    "# Load Random Forest\n",
    "print(\"\\n[1/3] Loading Random Forest...\")\n",
    "rf_model_data = joblib.load('Random-Forest/checkpoints/phishing_detector/rf_phishing_detector.joblib')\n",
    "rf_model = rf_model_data['model']\n",
    "rf_scaler = rf_model_data['scaler']\n",
    "print(\"‚úì Random Forest loaded\")\n",
    "\n",
    "# Load XGBoost\n",
    "print(\"\\n[2/3] Loading XGBoost...\")\n",
    "xgb_pipeline = joblib.load('XgBoost/phishing_text_model.joblib')\n",
    "print(\"‚úì XGBoost loaded\")\n",
    "\n",
    "# Load LLM (if GPU available)\n",
    "print(\"\\n[3/3] Loading LLM-GRPO...\")\n",
    "if GPU_AVAILABLE:\n",
    "    from unsloth import FastLanguageModel\n",
    "    import torch\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    llm_model, llm_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"AlexanderLJX/phishing-detection-qwen3-grpo\",\n",
    "        max_seq_length=2048,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(llm_model)\n",
    "    print(\"‚úì LLM-GRPO loaded\")\n",
    "else:\n",
    "    llm_model = None\n",
    "    llm_tokenizer = None\n",
    "    print(\"‚ö† LLM-GRPO skipped (no GPU)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úì ALL MODELS LOADED!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction functions\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Import feature extraction modules\n",
    "from feature_extraction_rf import extract_features_from_email as extract_rf_features\n",
    "from feature_extraction_text import TextFeatureExtractor\n",
    "\n",
    "xgb_extractor = TextFeatureExtractor()\n",
    "\n",
    "def predict_random_forest(subject, body):\n",
    "    \"\"\"Predict using Random Forest\"\"\"\n",
    "    features = extract_rf_features(subject, body)\n",
    "    features_scaled = rf_scaler.transform([features])\n",
    "    prediction = rf_model.predict(features_scaled)[0]\n",
    "    probability = rf_model.predict_proba(features_scaled)[0]\n",
    "    \n",
    "    return {\n",
    "        'model': 'Random Forest',\n",
    "        'prediction': 'Phishing' if prediction == 1 else 'Legitimate',\n",
    "        'confidence': float(max(probability)),\n",
    "        'phishing_probability': float(probability[1])\n",
    "    }\n",
    "\n",
    "def predict_xgboost(subject, body):\n",
    "    \"\"\"Predict using XGBoost\"\"\"\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame([{'subject': subject, 'body': body}])\n",
    "    prediction = xgb_pipeline.predict(df)[0]\n",
    "    probability = xgb_pipeline.predict_proba(df)[0]\n",
    "    \n",
    "    return {\n",
    "        'model': 'XGBoost',\n",
    "        'prediction': 'Phishing' if prediction == 1 else 'Legitimate',\n",
    "        'confidence': float(max(probability)),\n",
    "        'phishing_probability': float(probability[1])\n",
    "    }\n",
    "\n",
    "def predict_llm(subject, body):\n",
    "    \"\"\"Predict using LLM-GRPO\"\"\"\n",
    "    if not GPU_AVAILABLE or llm_model is None:\n",
    "        return {\n",
    "            'model': 'LLM-GRPO',\n",
    "            'prediction': 'N/A (GPU required)',\n",
    "            'confidence': 0.0,\n",
    "            'explanation': 'GPU not available'\n",
    "        }\n",
    "    \n",
    "    email_content = f\"Subject: {subject}\\n\\n{body[:1000]}\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are an expert cybersecurity analyst specializing in phishing email detection.\n",
    "Analyze the given email carefully and provide your reasoning.\n",
    "Place your analysis between <start_analysis> and <end_analysis>.\n",
    "Then, provide your classification between <CLASSIFICATION></CLASSIFICATION>.\n",
    "Respond with either \"PHISHING\" or \"LEGITIMATE\".\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Analyze this email:\\n\\n{email_content}\"}\n",
    "    ]\n",
    "    \n",
    "    inputs = llm_tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    outputs = llm_model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract classification\n",
    "    match = re.search(r'<CLASSIFICATION>(.+?)</CLASSIFICATION>', response, re.DOTALL)\n",
    "    if match:\n",
    "        classification = match.group(1).strip().upper()\n",
    "        prediction = 'Phishing' if 'PHISHING' in classification else 'Legitimate'\n",
    "    else:\n",
    "        prediction = 'Unknown'\n",
    "    \n",
    "    # Extract reasoning\n",
    "    reasoning_match = re.search(r'<start_analysis>(.+?)<end_analysis>', response, re.DOTALL)\n",
    "    explanation = reasoning_match.group(1).strip() if reasoning_match else \"No explanation provided\"\n",
    "    \n",
    "    return {\n",
    "        'model': 'LLM-GRPO',\n",
    "        'prediction': prediction,\n",
    "        'confidence': 0.95 if prediction != 'Unknown' else 0.0,\n",
    "        'explanation': explanation\n",
    "    }\n",
    "\n",
    "def predict_ensemble(subject, body):\n",
    "    \"\"\"Predict using all models and combine results\"\"\"\n",
    "    rf_result = predict_random_forest(subject, body)\n",
    "    xgb_result = predict_xgboost(subject, body)\n",
    "    llm_result = predict_llm(subject, body) if GPU_AVAILABLE else None\n",
    "    \n",
    "    # Weighted voting\n",
    "    rf_vote = 0.25 if rf_result['prediction'] == 'Phishing' else 0\n",
    "    xgb_vote = 0.35 if xgb_result['prediction'] == 'Phishing' else 0\n",
    "    llm_vote = 0.40 if (llm_result and llm_result['prediction'] == 'Phishing') else 0\n",
    "    \n",
    "    total_vote = rf_vote + xgb_vote + llm_vote\n",
    "    ensemble_prediction = 'Phishing' if total_vote >= 0.5 else 'Legitimate'\n",
    "    \n",
    "    return {\n",
    "        'ensemble_prediction': ensemble_prediction,\n",
    "        'ensemble_score': total_vote,\n",
    "        'random_forest': rf_result,\n",
    "        'xgboost': xgb_result,\n",
    "        'llm_grpo': llm_result if llm_result else 'N/A'\n",
    "    }\n",
    "\n",
    "print(\"‚úì Prediction functions created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Test Your Own Emails <a name=\"test\"></a>\n",
    "\n",
    "Try the models on your own email examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Test with a phishing email\n",
    "subject = \"URGENT: Your account will be suspended!\"\n",
    "body = \"\"\"Dear Valued Customer,\n",
    "\n",
    "We have detected unauthorized access to your account from an unknown location.\n",
    "Your account will be permanently suspended within 24 hours unless you verify your identity.\n",
    "\n",
    "Click here immediately: http://secure-verification-center.com/verify?id=12345\n",
    "\n",
    "Please provide:\n",
    "- Your username\n",
    "- Your password\n",
    "- Your social security number\n",
    "\n",
    "Failure to comply will result in permanent account deletion.\n",
    "\n",
    "Security Team\n",
    "ABC Bank\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING PHISHING EMAIL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSubject: {subject}\")\n",
    "print(f\"\\nBody: {body[:200]}...\")\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "\n",
    "result = predict_ensemble(subject, body)\n",
    "\n",
    "print(f\"\\nüéØ ENSEMBLE PREDICTION: {result['ensemble_prediction']}\")\n",
    "print(f\"   Confidence Score: {result['ensemble_score']:.2f}\")\n",
    "print(\"\\nIndividual Model Results:\")\n",
    "print(f\"  ‚Ä¢ Random Forest: {result['random_forest']['prediction']} ({result['random_forest']['confidence']:.2%})\")\n",
    "print(f\"  ‚Ä¢ XGBoost: {result['xgboost']['prediction']} ({result['xgboost']['confidence']:.2%})\")\n",
    "if result['llm_grpo'] != 'N/A':\n",
    "    print(f\"  ‚Ä¢ LLM-GRPO: {result['llm_grpo']['prediction']} ({result['llm_grpo']['confidence']:.2%})\")\n",
    "    print(f\"\\nüí° LLM Explanation: {result['llm_grpo']['explanation'][:300]}...\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Test with a legitimate email\n",
    "subject = \"Team Meeting Tomorrow at 2 PM\"\n",
    "body = \"\"\"Hi Team,\n",
    "\n",
    "Just a reminder that we have our weekly sync tomorrow at 2 PM in Conference Room B.\n",
    "\n",
    "Agenda:\n",
    "1. Project status updates\n",
    "2. Q4 planning\n",
    "3. Open discussion\n",
    "\n",
    "Please bring your laptops as we'll be reviewing the latest designs.\n",
    "\n",
    "See you all tomorrow!\n",
    "\n",
    "Best regards,\n",
    "Sarah\n",
    "Project Manager\"\"\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING LEGITIMATE EMAIL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSubject: {subject}\")\n",
    "print(f\"\\nBody: {body[:200]}...\")\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "\n",
    "result = predict_ensemble(subject, body)\n",
    "\n",
    "print(f\"\\nüéØ ENSEMBLE PREDICTION: {result['ensemble_prediction']}\")\n",
    "print(f\"   Confidence Score: {result['ensemble_score']:.2f}\")\n",
    "print(\"\\nIndividual Model Results:\")\n",
    "print(f\"  ‚Ä¢ Random Forest: {result['random_forest']['prediction']} ({result['random_forest']['confidence']:.2%})\")\n",
    "print(f\"  ‚Ä¢ XGBoost: {result['xgboost']['prediction']} ({result['xgboost']['confidence']:.2%})\")\n",
    "if result['llm_grpo'] != 'N/A':\n",
    "    print(f\"  ‚Ä¢ LLM-GRPO: {result['llm_grpo']['prediction']} ({result['llm_grpo']['confidence']:.2%})\")\n",
    "    print(f\"\\nüí° LLM Explanation: {result['llm_grpo']['explanation'][:300]}...\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive: Enter your own email!\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"INTERACTIVE EMAIL TESTING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Text input widgets for Colab\n",
    "if IN_COLAB:\n",
    "    print(\"\\nEnter your email details below:\")\n",
    "    custom_subject = input(\"Subject: \")\n",
    "    print(\"Body (enter your text, then press Ctrl+D or Ctrl+Z when done):\")\n",
    "    custom_body_lines = []\n",
    "    try:\n",
    "        while True:\n",
    "            line = input()\n",
    "            custom_body_lines.append(line)\n",
    "    except EOFError:\n",
    "        pass\n",
    "    custom_body = '\\n'.join(custom_body_lines)\n",
    "    \n",
    "    if custom_subject and custom_body:\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"Analyzing your email...\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        result = predict_ensemble(custom_subject, custom_body)\n",
    "        \n",
    "        print(f\"\\nüéØ ENSEMBLE PREDICTION: {result['ensemble_prediction']}\")\n",
    "        print(f\"   Confidence Score: {result['ensemble_score']:.2f}\")\n",
    "        print(\"\\nIndividual Model Results:\")\n",
    "        print(f\"  ‚Ä¢ Random Forest: {result['random_forest']['prediction']} ({result['random_forest']['confidence']:.2%})\")\n",
    "        print(f\"  ‚Ä¢ XGBoost: {result['xgboost']['prediction']} ({result['xgboost']['confidence']:.2%})\")\n",
    "        if result['llm_grpo'] != 'N/A':\n",
    "            print(f\"  ‚Ä¢ LLM-GRPO: {result['llm_grpo']['prediction']} ({result['llm_grpo']['confidence']:.2%})\")\n",
    "            print(f\"\\nüí° LLM Explanation: {result['llm_grpo']['explanation']}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö† No email entered\")\n",
    "else:\n",
    "    print(\"\\n‚ö† Interactive input only works in Colab\")\n",
    "    print(\"Use the previous cells with custom subject/body variables instead\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ‚úÖ Training Random Forest and XGBoost models\n",
    "2. ‚úÖ Evaluating pre-trained LLM-GRPO model\n",
    "3. ‚úÖ Comparing all 3 models with visualizations\n",
    "4. ‚úÖ Interactive API for testing emails in Colab\n",
    "\n",
    "**Key Findings:**\n",
    "- LLM-GRPO achieves highest accuracy (~99%) with explainability\n",
    "- XGBoost provides excellent accuracy (~89%) without GPU requirement\n",
    "- Random Forest offers fast, interpretable baseline (~82%)\n",
    "- Ensemble combines strengths of all models\n",
    "\n",
    "**Next Steps:**\n",
    "- Try more of your own emails in the interactive section\n",
    "- Experiment with different ensemble weights\n",
    "- Deploy as a production API using FastAPI\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
